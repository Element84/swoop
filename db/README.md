# `swoop-db`

The swoop database schema is managed via `dbmate`. `dbmate` will apply all
migration files found in the `./migrations` directory. We wrap the [upstream
dbmate tool](https://github.com/amacneil/dbmate) with a bash script
(`./dbmate`) to enforce a custom workflow where `./schema.sql` is not a "schema
only" dump made after applying migrations but is instead a "manually-created
reference schema" not generated by any `dbmate` commands.

The `dbmate` help is overridden by the wrapper script and includes details
about what is overridden and how. The main things of note:

* `./schema.sql` is not automatically dumped by `dbmate` commands
* `create` has an option to load a schema into the database
* `verify` is a new command that will check differences between `./schema.sql`
  and the schema generated by applying all of the migration files
* `test` is a new command (specific to swoop) to run the database tests

The `dbmate` wrapper can be run from the local OS against the postgres docker
container, but is typically easier to run within the container itself. See
details below.

### What is a "schema"?

Some have suggested that schema is an overloaded term, so it makes sense to
better define what that means.

In the context of postgres tooling like `pg_dump`, a schema is the structure of
the database including any types, functions, or proceedures (among,
potentially, many other things). With this definition perhaps it is better said
what a schema does not include: tables rows. Thus, a "schema" in this view is
everything but the data.

This perspective ignores the fact that some data may actually be part of the
structure of the database and required for proper database operations. Such
data should be considered different than data inserted by applications using
the database, and would therefore be considered an aspect of the schema we need
to track.

Moreover, some "schema" is actually generated from other commands, such that
the set of sql commands required to reproduce a given "schema" may actually be
much more limited than the result of running those commands. In an sense this
is like any build process where the minimal set of source artifacts actually
produce much more output than went in as they are built into the output
artifacts. In such a case, the accepted best practice is to track only that
minimal set of inputs, as the rest can be generated again at build time.

A clear example of this could be something like enabling a database extension.
The command to do so would be something like `CREATE EXTENSION
<extension_name>;`. When running a schema-only dump with `pg_dump`, all tables,
types, and other non-data items created in the database from that command would
be present in the output. But from our perpective as application developers, we
don't really care what the extension created, we just care to know we need to
run `CREATE EXTENSION <extension_name>;`. Therefore we should only track that
single command in our schema.

In these ways the schema we track in `./schema.sql` is different than what one
gets running `pg_dump` and exporting only the schema. And in this way
`./schema.sql` is different than what `dbmate` would dump into said file. But
the content we end up with in `./schema.sql` is much more useful for our
purposes.

### Why wrap `dbmate`?

The above difference in schema definitions is the reason. For more on the idea
and intent behind the dbmate wrapper, see [this dbmate
discussion](https://github.com/amacneil/dbmate/discussions/433).  The choice to
use a wrapper here is simply a pragmatic one; long-term either merging this
behaivor into `dbmate` or creating a dedicated cli tool that uses `dbmate` as a
library is preferred.

### What does it mean if the schema and migrations are out of sync?

The `./schema.sql` file represents what we want a new instance of the database
to look like, whereas the migrations are a way to capture what operations need
to be done to update an existing database from an older schema state to a new
one. Therefore, when we want make changes to `./schema.sql`, we need a
corresponding migration(s) to update existing databases with the older state.

Or, if we approach this from the other way around: if we make a migration to
make changes to existing databases, we also need to update the `./schema.sql`
in a corresponding manner.

In the event that changes are made to `./schema.sql` without a migraiton also
making those changes, or if we have a migration and fail to update
`./schema.sql`, then the schema and the migrations are out of sync. The
`verify` command added by the `dbmate` wrapper is used to detect this condition
and will provide a diff to help resolve any inconsistencies.

## Extensions

`swoop-db` makes use of two postgres extensions:

* `pg_partman`: an automated table partition manager
* `pgtap`: a postgres-native testing framework

## Database testing with docker

`./Dockerfile` defines the build steps for a database test container. The
container includes the requsite postgres extensions and any other required
utilities like `dbmate` and `pg_prove`.  As the Dockerfile builds an image with
all the database dependencies with fixed versions, using docker with that image
is strongly recommended for all testing to help guarantee consistency between
developers (running postgres in another way is fine if desired, but does
require that the necessary extensions and utilities are installed, and that the
connection information is correctly configured for tooling).

To make using the docker container more convenient, a `docker-compose.yml` file
is provided in the project root. The repo contents are mounted as `/swoop`
inside the container to help facilitate database operations and testing using
the included utilities. For example, to bring up the database and run the
tests:

```shell
# load the .env vars
source .env

# bring up the database container in the background
#   --build  forces rebuild of the container in case changes have been made
#   -V       recreates any volumes instead of reusing data
#   -d       run the composed images in daemon mode rather than in the foreground
docker compose up --build -V -d

# create the database and apply all migrations
docker compose exec postgres dbmate up

# run the database tests
docker compose exec postgres dbmate test db/tests/

# connect to the database with psql
docker compose exec postgres psql -U postgres swoop
```

To verify the schema and migrations match:

```shell
# drop an existing database to start clean
docker compose exec postgres dbmate drop

# run the verification; any diff indicates schema/migrations out-of-sync
docker compose exec postgres dbmate verify
```

To stop the `compose`d container(s):

```shell
docker compose down
```

### Adding a migration

Use `dbmate` if needing to create a new migration file:

```shell
docker compose exec postgres dbmate new <migration_name>
```

### Adding database tests

Database tests should be added as `.sql` files in the `./tests` directory.
Follow the pattern of the existing test files. It's best to keep each file
short and focused with a descriptive name. For more about the `pgtap` test
framework see [the documentation](https://pgtap.org/documentation.html).

## pre-commit hooks related to the database

We use `sqlfluff` for linting sql. See the root `.sqlfluff` config file and the
command defined in the `.pre-commit-config.yaml` for more information. Note
that the tool is a bit slow and somewhat inaccurate at times; it is better than
nothing but we should not hesitate to replace it with a better option if one
becomes available.
